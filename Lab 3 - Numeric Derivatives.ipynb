{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 - Numeric Derivatives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 1: Create and Examine Forward Difference Method "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import functions to start\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First, examine the code below which has implemented the forward difference algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardDiff(x,y):\n",
    "    h=x[1]-x[0] #find h based on delta x of data points\n",
    "    yprime=np.diff(y)/h # get forward differences \n",
    "    xdiff=x[:-1] # find corresponding x differences \n",
    "    return xdiff, yprime #return the arrays of forward difference values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the performance of an example function: $y = 3x^2+2x$. Note we are picking a polynomial here becuase we can easily compute the analytic derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid of x values and evaluate our function to get y(x)\n",
    "x = np.arange(-10,10)\n",
    "y = 3*x**2+2*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the numeric derivative. Also define the known analytic derivative: $y'=6x+2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find analytic derivative using our function, specify h to be close to our delta x in the data grid\n",
    "xalt,dy = forwardDiff(x,y)              ### EDIT - Removed 3rd argument which is not needed here\n",
    "\n",
    "#define the known analytic derivative for comparison - use the xalt array so that y prime and y will have the same length\n",
    "yprime = 6*xalt+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create plot\n",
    "plt.figure()\n",
    "plt.plot(xalt,dy,label='Forward Diff')\n",
    "plt.plot(xalt,yprime,label='Actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the maximum error between the forward difference and the actual solution from the previous plot. Compare with the value of the step size $h$ to determine if the error is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_error = max(abs(yprime - dy))\n",
    "print(f'The maximum error is: {max_error:.2e}')\n",
    "\n",
    "max_error_rel = max(abs(yprime - dy)/yprime)/100.                   ### EDIT - Added relative error calc. here\n",
    "print(f'The maximum relative error is: {max_error_rel:.3f} %')  \n",
    "\n",
    "stepsize=x[1]-x[0]\n",
    "print(f'The stepsize is: {stepsize:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While these values are not exactly the same, we can see that they are comparable which agrees with expectation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next let's look at another function that has slightly more complex behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider $y = 5x^4+7x^2-6x$ and repeat the steps above taking the numeric derivative and the analytic derivative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid of x values and evaluate our function to get y(x)\n",
    "x = np.arange(-10,10,1E-1)\n",
    "y = 5*x**4+7*x**2-6*x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the numeric derivative. Also define the known analytic derivative. I'll leave this for you to 'fill in the blank'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find analytic derivative using our function, specify h to be close to our delta x in the data grid\n",
    "xalt,dy = forwardDiff(x,y)      ### EDIT - Removed 3rd argument which is not needed here\n",
    "\n",
    "#define the known analytic derivative for comparison - use the xalt array so that y prime and y will have the same length\n",
    "yprime =   #add your code here...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the results and compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create plot\n",
    "plt.figure()\n",
    "plt.plot(xalt,dy,label='Forward Diff')\n",
    "plt.plot(xalt,yprime,label='Actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the maximum error between the forward difference and the actual solution from the previous plot. Compare with the value of the step size $h$ to determine if the error is reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_error = max(abs(yprime - dy))\n",
    "print(f'The maximum error is: {max_error:.2e}')\n",
    "\n",
    "max_error_rel = max(abs(yprime - dy)/yprime)/100.                   ### EDIT - Added relative error calc. here\n",
    "print(f'The maximum relative error is: {max_error_rel:.3f} %')  \n",
    "\n",
    "stepsize=x[1]-x[0]\n",
    "print(f'The stepsize is: {stepsize:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that relative error is still comparable to before even though max_error has grown. This is due to the functional form we are dealing with in this example. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 2: The Central Difference Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start by examining the code below which has implemented the central difference algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The function provided below computes the descrete version of the central difference. Take a look at the code to verify that it works (you might also want to check out the functionality of np.roll() as it is used here). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the code to verify that it does what you'd expect\n",
    "def CentralDiff(x,y): \n",
    "    h=x[1]-x[0] #find h based on delta x of data points\n",
    "    forward = np.roll(y,-1) #forward step\n",
    "    backward = np.roll(y,+1) #backward step\n",
    "    dycd=(forward-backward)/(2*h) #perform difference\n",
    "    dycd=dycd[1:-1] #trim boundaries\n",
    "    dxcd=x[1:-1] #trim boundaries                          ### EDIT: change dx -> x. Update boundaries for trim\n",
    "    return dxcd, dycd #return the array of the central difference values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following from the previous example with the forward derivative, take a numeric derivative of our original function $y = 3x^2+2x$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid of x values and evaluate our function to get y(x)\n",
    "x = np.arange(-10,10,1E-1)\n",
    "y = 3*x**2+2*x\n",
    "\n",
    "# find analytic derivative using our function, specify h to be close to our delta x in the data grid\n",
    "xalt,dy = CentralDiff(x,y)\n",
    "\n",
    "#define the known analytic derivative for comparison - use the xalt array so that y prime and y will have the same length\n",
    "yprime = 6*xalt+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create plot\n",
    "plt.figure()\n",
    "plt.plot(xalt,dy,label='Forward Diff')\n",
    "plt.plot(xalt,yprime,label='Actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check error and compare with earlier result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_error = max(abs(yprime - dy))\n",
    "print(f'The maximum error is: {max_error:.2e}')\n",
    "\n",
    "max_error_rel = max(abs(yprime - dy)/yprime)/100.                   ### EDIT - Added relative error calc. here\n",
    "print(f'The maximum relative error is: {max_error_rel:.3f} %') \n",
    "\n",
    "stepsize=x[1]-x[0]\n",
    "print(f'The stepsize is: {stepsize:.2e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note this is a significant improvement!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 3: Choice of Step Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we modified our forward difference formula so that we can specify step size? What could go wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forwardDiff2(x,y,h=1E-5):\n",
    "    yprime=np.diff(y)/h # get forward differences \n",
    "    xdiff=x[:-1] # find corresponding x differences \n",
    "    return xdiff, yprime #return the arrays of forward difference values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuition might tell you that smaller step size is better, however this is dangerous! Note the default $h$ in our function is now much smaller than it was in the earlier version. Let's investigate what this does..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test this new implementation on our function $y = 3x^2+2x$ and examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create grid of x values and evaluate our function to get y(x)\n",
    "x = np.arange(-10,10,1E-1)\n",
    "y = 3*x**2+2*x\n",
    "\n",
    "# find analytic derivative using our function, specify h to be close to our delta x in the data grid\n",
    "xalt,dy = forwardDiff2(x,y)\n",
    "\n",
    "#define the known analytic derivative for comparison - use the xalt array so that y prime and y will have the same length\n",
    "yprime = 6*xalt+2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and compare the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create plot\n",
    "plt.figure()\n",
    "plt.plot(xalt,dy,label='Forward Diff')\n",
    "plt.plot(xalt,yprime,label='Actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the shocking difference here... With the small $h$ choice, we're getting dominated by round off errors!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat the process above, but try a few different specified h values (e.g, 1E-2, 5E-1, 1E-1, 0.5) and compare the results. Which works best and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example code\n",
    "xalt,dy = forwardDiff2(x,y,h=3E-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create plot\n",
    "plt.figure()\n",
    "plt.plot(xalt,dy,label='Forward Diff')\n",
    "plt.plot(xalt,yprime,label='Actual')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this part, I hope you can easily see that choice of $h$ is extremely important and can lead to junk results if you are not careful! Best practice is always using $h$ comparable to $\\Delta x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activity 4: Working with Noisy Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In real world environments, data collected always has noise. As discussed in class, working with noisy data can be troublesome when taking a numeric derivative. Here we will explore techniques for handling noisy data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine we are collecting data for a system whose position can be described by the $\\sin(x)$ function (think oscillitory things like mass-springs, etc.). Suppose the system has some sort of higher order frequency behavior, which is interpreted as noise for the sake of the experiment. The noise can be modeled as $\\epsilon \\sin(\\omega x)$, where $\\epsilon$ is the noise amplitude and $\\omega$ is the angular frequency for the noise term. Putting this together gives us the form: $f(x) = A \\sin(x) + \\epsilon \\sin(\\omega x)$. A function that models the system is provided below, which takes arguments $A, \\epsilon, \\omega$, and an array of $x$ values and returning an array $f(x)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NoisySine(x,A,eps,w):\n",
    "    fx=A*np.sin(x)+eps*np.sin(w*x)\n",
    "    return fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the function plotted to get a better sense of it. As an example, we will use values $A = 5, \\epsilon = 1.0, \\omega=30$ over the range $[0,2\\pi]$. For comparison purposes, we'll also include an entry for the model that does not have a noise term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0,2*np.pi,1001) # array of x values  ### EDIT - updated parameters\n",
    "\n",
    "### EDIT - updated parameters\n",
    "A = 5\n",
    "eps = 0.5\n",
    "omega  = 15\n",
    "\n",
    "example_data=NoisySine(x,A,eps,omega) # evaluate our noisy sine function with given parameters     ### EDIT - updated parameters\n",
    "\n",
    "no_noise=NoisySine(x,A,0.0,omega) # create example of sine function without added noise -> set epsilon to zero    ### EDIT - updated parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and comparre the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x,example_data,label='Example Data')\n",
    "plt.plot(x,no_noise,label='No Noise')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's consider taking a derivative. We can easily show that our function $f(x)$ with the given noise term has a derivative $f'(x) = A \\cos(x) + \\omega \\epsilon \\cos(\\omega x)$. Note the behavior if $\\omega$ is large, the amplitude of the error term in the derivative can be much larger than the amplitude of the 'true' function. Let's take a closer look here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_prime=A*np.cos(x)+omega*eps*np.cos(omega*x) #noisy function analytic derivative         ### EDIT - updated parameters\n",
    "\n",
    "noiseless_prime = A*np.cos(x) # analytic derivative of no noise function            ### EDIT - updated parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and compare the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x,data_prime,label=r'$y^{\\prime}$ Example Data')\n",
    "plt.plot(x,noiseless_prime,label=r'$y^{\\prime}$ No Noise')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Smoothing\n",
    "Smooting is a common technique when working with noisy data. We will look at this to improve the results of the last few cells. Start by examining the function below, which performs smoothing on the data $f(x)$ by taking an average of the input array from $iâˆ’n$ to $i+n$, for each element $i$ with smoothing radius $n$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(x,n):\n",
    "    A=np.zeros((n,len(x)))\n",
    "    for i in range(0,n):\n",
    "        A[i]=np.roll(x,i)\n",
    "    sdata=np.mean(A,axis=0)\n",
    "    sdata=np.roll(sdata,np.int32(-n/2))         ### EDIT - change np.int() to np.int32()\n",
    "    return sdata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there is certainly more than one way to perform smoothing, check out the behavior of $np.roll()$ which is very useful for our purpose here. Also, note that we have assumed the data wraps around on the endpoints, which is fine for our sine function, but be careful if our data is not perodic in nature. You will need different boundary conditions in those events!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer back to your earlier work with the example function $f(x) = A \\sin(x) + \\epsilon \\sin(\\omega x)$. Create a plot using the same values of $A, \\epsilon,$ and $\\omega$ over the range $[0,2\\pi]$ like before. This time include an additional curve for a smoothed version of this function with $n=51$. for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smoothed_data=smooth(example_data,51) #run smoothing on the example data       ### EDIT - updated parameters\n",
    "\n",
    "\n",
    "#Plot and compare the smoothed_data, example_data, and no_noise\n",
    "plt.figure()\n",
    "### Add code here ####\n",
    "\n",
    "\n",
    "#####################\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use our earlier function for calculating the numeric derivative via the central difference. Use this to find the derivative of the unsmoothed function (example_data) and the smoothed function (smoothed_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### add your code here            ### EDIT: Added more example code\n",
    "\n",
    "# perform central difference on example data\n",
    "cdiff_x,cdiff_y=CentralDiff(... , ...)\n",
    "\n",
    "# smooth example data - update value as needed for smoothing parameter\n",
    "smthd_y = smooth(... , ...)\n",
    "\n",
    "# perform central diff on smoothed data\n",
    "sm_cd_x,sm_cd_y=CentralDiff(x, smthd_y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Create a plot below to compare the results. Additionally, include a curve for the analtic derivative of the noisy function ($f'(x) = A \\cos(x) + \\omega \\epsilon \\cos(\\omega x)$ similar to before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "### Add code here ####\n",
    "\n",
    "\n",
    "\n",
    "#####################\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the cells above trying different values of smoothing. Note that a good choice of smoothing can allow the numeric derivative to  approximate the `true' function derivative as if there was not noise. You will have to experiment with values to find the optimal smoothing parameter. There is an N value between 50 and 75 that yeilds almost flawless results. Try finding it :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
