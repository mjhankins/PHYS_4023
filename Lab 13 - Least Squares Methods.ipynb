{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6884d2bc-8dbb-4a6b-8dee-eea94e800410",
   "metadata": {},
   "source": [
    "# Lab 13: Least Squares Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983d062-33dc-44ad-b0b6-0193da068862",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import needed libraries\n",
    "import numpy as np\n",
    "from numpy.linalg import lstsq\n",
    "from scipy.optimize import curve_fit\n",
    "import scipy.linalg as la\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edc8c12-2021-41c2-95e1-c1f2545b4138",
   "metadata": {},
   "source": [
    "### Activity 1: Ordinary Least Squares Analysis with Hooke's Law"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90ecb0d-08d2-48bf-81ed-914530771ddc",
   "metadata": {},
   "source": [
    "Suppose we have an experiment measuring the spring constant similar to the example in class. We want to determine the spring constant $k$ given a set of force ($F_{sp}$) and displacement ($\\delta x$) measurements. Recall that Hooke's law tells us that these are related via:\n",
    "$$\n",
    "F_{sp}= - k \\Delta x\n",
    "$$\n",
    "which is clearly a linear relationship, and ordinary (or linear) least squares is appropriate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186c1760-d5a8-46d0-8152-c18a48d02698",
   "metadata": {},
   "source": [
    "We will start by generating some simulated experimental data - note we could use real measurments here, but for simplicity we will generate simulated data with some noise, which will allow us later to adjust noise amplitude on the 'fly' so we can examine how that impacts the process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d619e67b-a1ad-42f6-af30-555c166e450c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 'Experimental Data'\n",
    "\n",
    "#crate list of x values to consider\n",
    "x = np.linspace(0, 4, 201)\n",
    "\n",
    "#specify spring constant\n",
    "k = 10\n",
    "\n",
    "#specify the noise amplitude - can be adjusted as desired\n",
    "noise_amp = 3\n",
    "\n",
    "#create the 'y' data using Hooke's Law and adding a noise term using a random number generator\n",
    "Fspring = k*x + x * np.random.random(len(x))*noise_amp "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f6416-f526-48b0-b31e-d4e1afb1e8b6",
   "metadata": {},
   "source": [
    "Plot the data we just created and ensure it appears as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31262b1c-7be6-419f-a584-da5915159732",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.plot(x, Fspring, 'b.')\n",
    "\n",
    "plt.ylabel(r'F$_{spring}$ (N)')\n",
    "plt.xlabel(r'$\\Delta x$ (cm)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d879f92d-bed8-48ff-8d08-7cb7da388074",
   "metadata": {},
   "source": [
    "Let's perform ordinary least squares as a linar algebra problem. Recall that for a fitted model in linear least squares we assume a function $f(x,\\alpha_i) = \\alpha_1*x + \\alpha_0$. The alpha parameters form a column vector $\\alpha$ which can be found by solving:\n",
    "$$\n",
    "(X^TX)^{-1}X^T Y = \\alpha\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81d3807-2a07-4a5f-9a71-c767f7b22b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#start by constructing the necissary matricies\n",
    "\n",
    "# assemble matrix X from our x data points\n",
    "X = np.vstack([x, np.ones(len(x))]).T\n",
    "\n",
    "# Create Y as a column vector of the spring force values\n",
    "Y = Fspring[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3587578e-a61c-471a-a2c2-2ac257bc897a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now do the matrix multiplication to solve for \\alpha\n",
    "\n",
    "alpha = np.linalg.inv(X.T@X)@X.T@Y\n",
    "print('The optimized parameters are: ', alpha.T[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533b5824-5c3c-4e0e-8202-f92ca638614e",
   "metadata": {},
   "source": [
    "Take a moment to calculate the percent difference with respect to 'k', given that we know the 'true' value from earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa8d414-13b5-4db5-9100-211bcbdfd2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e75f0-5184-4375-b344-a345eab67e46",
   "metadata": {},
   "source": [
    "At the start of the notebook, we imported the `numpy.linalg` package with `lstsq` function. Try implementing it below to verify that our answer agrees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef0979d-5f15-4d52-a691-7d8638a7523f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31ec8db-7244-46b0-86d7-14e90455b879",
   "metadata": {},
   "source": [
    "Finally, let's plot the result with our original data to see that the agreement witht the fitted curve is good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50695aa4-cbfe-4853-9dda-01389886e670",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "### ADD YOUR CODE BELOW ###\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.ylabel('F_spring (N)')\n",
    "plt.xlabel('Displacement (cm)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279b386f-591e-4e73-9eda-adac936e2792",
   "metadata": {},
   "source": [
    "To complete this lab activity, try repeating the above cells (starting from the top) with different values of the noise amplitude parameter. Take a few moments to study how this impacts the percent error calculation with a few 'test runs'. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8bca60-bf73-4ea7-805e-248aae128a8b",
   "metadata": {},
   "source": [
    "# Activity 2: Non-Linear Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a84537-ca8d-43c4-9c09-1deea6ec5c60",
   "metadata": {},
   "source": [
    "Suppose we have a set of data that visually seems to follow an exponential decay form, and we want to fit the function $f(t)=\\alpha_0 e^{\\alpha_1 t}$ to determine the parameters $\\alpha_i$. \n",
    "Let's look at a solution using the Gauss-Newton method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6dd2af-1503-45df-bd30-ea130bb247ae",
   "metadata": {},
   "source": [
    "Start by define a simple data set to be fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35bcdec-b6f0-4407-90cb-9d51b0664b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Data\n",
    "t = np.array([0.0, 1.0, 2.0, 3.0])\n",
    "y = np.array([2.0, 0.7, 0.3, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c700f1-7001-4582-a109-4c8e3eee92ee",
   "metadata": {},
   "source": [
    "Next, we need to define a few functions to make this proces easier, including the residual ($r$), and the Jacobian ($J$) for this functional form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef36d2-5ac2-4c14-82d3-7c9f1209fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual(x):\n",
    "    return y - x[0] * np.exp(x[1] * t)\n",
    "\n",
    "def jacobian(x):\n",
    "    return np.array([ -np.exp(x[1] * t), -x[0] * t * np.exp(x[1] * t)]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b0ef7-8c08-44fc-a3a4-2bf460d43738",
   "metadata": {},
   "source": [
    "We'll also want to look at our progress during the iteration. The function below will be handy in examining the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd69864-f503-4d98-80ac-48bce87cb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_guess(x,plot_cutoff=1E5):\n",
    "    norm2= la.norm(residual(x), 2)\n",
    "    print(\"Residual norm:\", norm2)\n",
    "    \n",
    "    if norm2<=plot_cutoff:\n",
    "        plt.plot(t, y, 'ro', markersize=8, clip_on=False)\n",
    "        T = np.linspace(t.min(), t.max(), 100)\n",
    "        Y = x[0] * np.exp(x[1] * T)\n",
    "        plt.plot(T, Y, 'b-')\n",
    "        plt.ylabel('y')\n",
    "        plt.xlabel('x')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee76c45-9084-41eb-b2cb-49e517bab7dd",
   "metadata": {},
   "source": [
    "Now let's examine the fitting. Recall that we must start with an intial guess for this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8a4a4d-958f-48ac-a9a8-88ef1fc9de4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#an initial guess to start with\n",
    "x = np.array([0.4, 2])  # array with alpha_0 and alpha_1\n",
    "\n",
    "#plot\n",
    "plt.figure()\n",
    "plot_guess(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6459a43b-33f5-4768-9f06-3b7091a1ddcf",
   "metadata": {},
   "source": [
    "Not a very satisfying result to start with, but that's why we need to iterate! Try using iteration on this process by updating our parameters each time following the form:\n",
    "$$\n",
    "\\alpha^{k+1} = \\alpha^k - (J^TJ)^{-1}J^T r(\\alpha^k)\n",
    "$$\n",
    "To perform this calculation more efficiently, let's take advantage of the fact that $(J^TJ)^{-1}J^T$ is the psuedo-inverse of $J$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c92cb4-fbf4-48d2-94d4-0056b9e43425",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set number of iterations to perform\n",
    "iterations = 5\n",
    "\n",
    "#execute iteration as a for loop so we can plot each iteration as we go\n",
    "for i in range(iterations):\n",
    "\n",
    "    #update parameters\n",
    "    x = x + np.linalg.pinv(jacobian(x))@-residual(x)\n",
    "\n",
    "    #plot\n",
    "    plt.figure()\n",
    "    plot_guess(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9189e41f-58f9-4ee4-90d8-71a715452c11",
   "metadata": {},
   "source": [
    "Note the above can get quite messy for many iterations showing the plot each time. We can use the 'plot_cutoff' argument in the function to only show 'good' fits (say residual norms<1). Now we can iterate many times and check for convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5754ea-d68c-4c32-b073-b913d63d4815",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial guess to start with\n",
    "x = np.array([0.4, 2])\n",
    "\n",
    "#set number of iterations\n",
    "iterations = 20\n",
    "\n",
    "#execute\n",
    "for i in range(iterations):\n",
    "\n",
    "    # update parameters\n",
    "    x = x + la.lstsq(jacobian(x), -residual(x))[0]\n",
    "\n",
    "    #plot\n",
    "    plt.figure()\n",
    "    plot_guess(x,plot_cutoff=1.0) #use extra option here to not plot fits unless they are relativly good "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fb6cba-61c2-4563-86ef-42f825a6b1fd",
   "metadata": {},
   "source": [
    "Notice the last few values of the residual norm indicate that we are converging to a solution. That said, it might be useful to have an implementation that stops early if the change in fitted solution one iteration to the next becomes less than 1%. Take a moment to think about this, and try implementing below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9276bd-01f5-4aab-b1a3-ced85b253010",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD YOUR CODE HERE ###\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37157a69-031e-459d-96ed-cffdc92c4d08",
   "metadata": {},
   "source": [
    "# Activity 3: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199e9cd-07d1-439f-95ab-2d9471bc0e55",
   "metadata": {},
   "source": [
    "We can use numpy to easily and efficiently implement polynomial regression for non-trivial functions. Let's examine a sample data set, which is clearly non-linear. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd05c400-6c42-46e1-9145-97e427538d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a set of test data and plot\n",
    "x_d = np.array([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
    "y_d = np.array([0, 0.8, 0.9, 0.1, -0.6, -0.8, -1, -0.9, -0.4])\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(x_d,y_d,'o')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9be9dd-072f-45b5-b3d6-36392fecba7d",
   "metadata": {},
   "source": [
    "Try performing the polynomial fit using `np.polyval()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f886fcc7-412c-45c8-bc8d-7c9b0d00561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear fitting function\n",
    "y_est = np.polyfit(x_d, y_d, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bf794a-0174-45f2-8ca8-dbc6366fc18a",
   "metadata": {},
   "source": [
    "plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35bb6f2-e284-46e2-9c3f-0d2bc4ff9a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.title(f'Polynomial order 1')\n",
    "\n",
    "plt.plot(x_d, y_d, 'o')\n",
    "plt.plot(x_d, np.polyval(y_est, x_d))\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5b7ba0-fe0b-4c4f-8b83-a70a9161ae86",
   "metadata": {},
   "source": [
    "This is not terribly satisfying, so let's try fitting with different powers of a polynomial and examine the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4413bc47-f5f5-4188-ac3c-e07b9dec3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#set maximum degree of polynomial to consider\n",
    "MaxPolyDeg = 10\n",
    "\n",
    "# loop through fitting and plot results\n",
    "for i in range(1, MaxPolyDeg+1):\n",
    "\n",
    "    #perform polyfit on order 'i'\n",
    "    y_est = np.polyfit(x_d, y_d, i)\n",
    "\n",
    "    #plot result\n",
    "    plt.figure()\n",
    "\n",
    "    plt.title(f'Polynomial order {i}')\n",
    "    \n",
    "    plt.plot(x_d, y_d, 'o')\n",
    "    plt.plot(x_d, np.polyval(y_est, x_d))\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d4abb-6e7e-4371-bc00-1ed37a03d3fa",
   "metadata": {},
   "source": [
    "Notice how the function fit improves with N. Let's explore this further:\n",
    "\n",
    "* Write a function that takes the fitted result for a given polynomial and calculates the square residuals\n",
    "* Perform polynomial fitting for the example data going up to at least a 10th order polynomial.\n",
    "* Examine the trend in the square residuals for each polynomial order.\n",
    "* Consider if we might be hitting a point where we are 'overfitting', and where this occurs. Does it make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270aa10f-c1eb-4f6f-9dd4-650c2d6f498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ADD CODE AND CELLS BELOW AS NEEDED ###\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
