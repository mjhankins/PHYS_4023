{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d14f7485-2d59-437e-930e-ce732f605f7a",
   "metadata": {},
   "source": [
    "# Lab 10  - Root Finding and Optimization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332688c-c88a-46c5-afca-91579b0d6921",
   "metadata": {},
   "source": [
    "In this activity, we will examine both root finding and optimization methods useful in computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5a77a1-9202-4ff7-8310-c65ce89190be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required functions\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import fsolve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db33d7b1-c977-46f9-8bbc-1cbdaff3f4d2",
   "metadata": {},
   "source": [
    "### Activity 1: Root finding with Bisection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65177a4d-edac-4264-ab6f-c0efe313bd11",
   "metadata": {},
   "source": [
    "below we have defined a function that performs bisection method for finding roots. Take a moment to review the code and verify that it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eade2735-d87a-42dd-b2de-b9cc425c506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bisection_method(f, a, b, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Find root of f(x) using bisection method.\n",
    "    \n",
    "    Parameters:\n",
    "        f: function to find root of\n",
    "        a, b: interval [a,b] where f(a) and f(b) have opposite signs\n",
    "        tol: tolerance for convergence\n",
    "        max_iter: maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        root approximation and number of iterations\n",
    "    \"\"\"\n",
    "    if f(a) * f(b) > 0:\n",
    "        raise ValueError(\"f(a) and f(b) must have opposite signs\")\n",
    "    \n",
    "    iterations = 0\n",
    "    while (b - a) / 2 > tol and iterations < max_iter:\n",
    "        c = (a + b) / 2\n",
    "        if f(c) == 0:\n",
    "            return c, iterations\n",
    "        elif f(a) * f(c) < 0:\n",
    "            b = c\n",
    "        else:\n",
    "            a = c\n",
    "        iterations += 1\n",
    "    \n",
    "    return (a + b) / 2, iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a75b46-2b5b-4932-b29d-99c5c1d8f0ac",
   "metadata": {},
   "source": [
    "Define a function that we will use to test the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9836c7e0-0c1a-4227-b685-332149615cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test function \n",
    "def f(x):\n",
    "    return x**3 - 2*x - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420cab7c-3e70-4bb7-877c-d386fd35d163",
   "metadata": {},
   "source": [
    "Use the function to find the root of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6aec5c7-5639-4295-aa53-9eb9cc6c2529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use function\n",
    "root_bisect, iter_bisect = bisection_method(f, 1, 3)\n",
    "\n",
    "#print results\n",
    "print(f\"Root found: x = {root_bisect:.10f}\")\n",
    "print(f\"f(x) = {f(root_bisect):.2e}\")\n",
    "print(f\"Iterations: {iter_bisect}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116a7ad-3770-422e-996a-9c990fd678ff",
   "metadata": {},
   "source": [
    "Check our answer with `scipy.optimize.fsolve()`. Note fsolve requires an initial guess, we'll put $x_0 = 1 $ as a 'reasonable' guess. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca1e355-b39e-40ae-9ad8-f8385066b70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial guess\n",
    "x0=1\n",
    "\n",
    "#run scipy.optimize fsolve on our funciton with the initial guess\n",
    "fs_result=fsolve(f,x0)\n",
    "\n",
    "#print result\n",
    "print(f\"fsolve result for root: {fs_result[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c714b0dd-a724-4394-b268-20216bf5e5f1",
   "metadata": {},
   "source": [
    "We can also take a look at the result by plotting. Let's do that as a way of verifying the found root."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0fc3b9-b957-4f44-bd98-c1b2529a4685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create x values to feed our function\n",
    "xs=np.arange(-4,4.,0.1)\n",
    "\n",
    "#evalute function\n",
    "ys=f(xs)\n",
    "\n",
    "#create plot\n",
    "fig,ax =plt.subplots(nrows=1, ncols=2,figsize=(9,4))\n",
    "\n",
    "#title\n",
    "ax[0].set_title(\"f(x) over range (-4,4)\")\n",
    "\n",
    "#plot function\n",
    "ax[0].plot(xs,ys,'r--')\n",
    "\n",
    "#plot a marker at our found root to verify\n",
    "ax[0].plot(root_bisect,0,'bx',ms=8)\n",
    "\n",
    "#add slolid line for x and y axis\n",
    "ax[0].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax[0].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"y\")\n",
    "\n",
    "\n",
    "#make zoom in plot\n",
    "ax[1].set_title(\"Zoom in Near Root\")\n",
    "\n",
    "#plot function\n",
    "ax[1].plot(xs,ys,'r--')\n",
    "\n",
    "#plot a marker at our found root to verify\n",
    "ax[1].plot(root_bisect,0,'bx',ms=10)\n",
    "\n",
    "#add slolid line for x and y axis\n",
    "ax[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax[1].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "#set zoom in region\n",
    "ax[1].set_xlim(1.8, 2.4)\n",
    "ax[1].set_ylim(-0.5,0.5)\n",
    "\n",
    "ax[1].set_ylabel(\"y\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb127c-39e4-41bd-800a-7069f721a0de",
   "metadata": {},
   "source": [
    "### Activity 2: Root Finding with Newton-Raphson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d2e2b-a5e5-46b7-98eb-958f048b148f",
   "metadata": {},
   "source": [
    "below we have defined a function that performs the Newton-Raphson method for finding roots. Take a moment to review the code and verify that it works as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79559ca1-3d7a-43cc-a0af-df4a851fbd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_raphson(f, f_prime, x0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Find root of f(x) using Newton-Raphson method.\n",
    "    \n",
    "    Parameters:\n",
    "        f: function to find root of\n",
    "        f_prime: derivative of f\n",
    "        x0: initial guess\n",
    "        tol: tolerance for convergence\n",
    "        max_iter: maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        root approximation and number of iterations\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    iterations = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        fx = f(x)\n",
    "        if abs(fx) < tol:\n",
    "            return x, iterations\n",
    "        \n",
    "        fpx = f_prime(x)\n",
    "        if fpx == 0:\n",
    "            raise ValueError(\"Derivative is zero. Newton-Raphson fails.\")\n",
    "        \n",
    "        x = x - fx / fpx\n",
    "        iterations += 1\n",
    "    \n",
    "    return x, iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c3146-aaf6-4fce-b455-93aed8734562",
   "metadata": {},
   "source": [
    "For this exercise, let's use the same function as in activity 1. Recall it was: $f(x) = x^3 - 2x - 5$. Newton-Raphson requires knowledge of the first derivative, which we can create below: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0bccb-3aef-4a52-a373-8be5de81b611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_prime(x):\n",
    "    return 3*x**2-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc3e40f-de19-478c-bfd6-5b6b597270e1",
   "metadata": {},
   "source": [
    "Now use our function to find the root:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ed9a24-2f98-4dba-8b11-21ad5f63288a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use the newton_raphson() function\n",
    "root_newton, iter_newton = newton_raphson(f, f_prime, 1.0)\n",
    "\n",
    "\n",
    "print(f\"Root found: x = {root_newton:.10f}\")\n",
    "print(f\"f(x) = {f(root_newton):.2e}\")\n",
    "print(f\"Iterations: {iter_newton}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcc6a68-6dd7-42c5-8532-34f8a261ecb9",
   "metadata": {},
   "source": [
    "Notice how much faster this converges to the desired result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68de389-87fb-4f1e-9616-d05477c18e2e",
   "metadata": {},
   "source": [
    "### Activity 3: Optimization with Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca4a1c8-a950-44f2-9922-9ddb9441d354",
   "metadata": {},
   "source": [
    "Recall our discussion of Gradient Desent as a method for finding extrema for a given function. Below we have defined a function that performs this method in the 1D case. Take a moment to examine the code and verify that it works as expected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d510b3c8-b2ab-4054-9228-4aceb94858f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, f_prime, x0, learning_rate=0.1, tol=1e-6, max_iter=1000):\n",
    "    \"\"\"\n",
    "    Find minimum of f(x) using gradient descent.\n",
    "    \n",
    "    Parameters:\n",
    "        f: function to minimize\n",
    "        f_prime: derivative of f\n",
    "        x0: initial guess\n",
    "        learning_rate: step size for updates\n",
    "        tol: tolerance for convergence\n",
    "        max_iter: maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        minimum location, function value, and number of iterations\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    iterations = 0\n",
    "    history = [x]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = f_prime(x)\n",
    "        x_new = x - learning_rate * grad\n",
    "        history.append(x_new)\n",
    "        \n",
    "        if abs(x_new - x) < tol:\n",
    "            return x_new, f(x_new), iterations, history\n",
    "        \n",
    "        x = x_new\n",
    "        iterations += 1\n",
    "    \n",
    "    return x, f(x), iterations, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe602b5d-3249-401f-890d-1d66bde2303d",
   "metadata": {},
   "source": [
    "For our optimization functions, let's consider a different function we'll call $g(x)$. We'll go ahead and define derivatives as needed for this part as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7290726b-5bbc-4046-903c-65612901fc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a test function for optimization: g(x) = x^2 - 4x + 4 (minimum at x=2)\n",
    "def g(x):\n",
    "    return x**2 - 4*x + 4\n",
    "\n",
    "def g_prime(x):\n",
    "    return 2*x - 4\n",
    "\n",
    "def g_double_prime(x):\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5b47b-0210-46ec-a318-cee91d96efc0",
   "metadata": {},
   "source": [
    "Perform function and print result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b264c319-b55d-42a3-9938-989c59e5e19e",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_gd, fmin_gd, iter_gd, hist_gd = gradient_descent(g, g_prime, 5.0, learning_rate=0.3)\n",
    "print(f\"Minimum at: x = {min_gd:.10f}\")\n",
    "print(f\"Function value: g(x) = {fmin_gd:.10f}\")\n",
    "print(f\"Iterations: {iter_gd}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86396cd9-a1f5-4a0b-8bbb-c7ab992ae132",
   "metadata": {},
   "source": [
    "We can check this result via one of two methods: 1. Analytic calculation (if function is differentiable) 2. Graphing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abbe50b-caee-423b-ae87-e4442458711b",
   "metadata": {},
   "source": [
    "Let's do both starting with the Analytic calculation. Recall extrema are defined where $g'(x)$ = 0. So let's look a this as a type of 'root finding' problem and call fsolve() on $g'(x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fe2f0e-e4b2-4ec9-8d3d-c0e57c183ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#state an initial guess\n",
    "x0 = 0.0\n",
    "\n",
    "fs_result=fsolve(g_prime,x0)\n",
    "\n",
    "#print result\n",
    "print(f\"fsolve result for optimization: {fs_result[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45838d1-caea-4e5a-8e06-81dbc8eaa107",
   "metadata": {},
   "source": [
    "And now for plotting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7748d7a8-5395-4507-b514-f5ad38f17e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create x values to feed our function\n",
    "xs=np.arange(-4,4.,0.1)\n",
    "\n",
    "#evalute function\n",
    "ys=g(xs)\n",
    "\n",
    "#create plot\n",
    "fig,ax =plt.subplots(nrows=1, ncols=2,figsize=(9,4))\n",
    "\n",
    "#title\n",
    "ax[0].set_title(\"g(x) over range (-4,4)\")\n",
    "\n",
    "#plot function\n",
    "ax[0].plot(xs,ys,'g--')\n",
    "\n",
    "#plot a marker at our found minimum point to verify\n",
    "ax[0].plot(min_gd,0,'mx',ms=8)\n",
    "\n",
    "#add slolid line for x and y axis\n",
    "ax[0].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax[0].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "ax[0].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"y\")\n",
    "\n",
    "\n",
    "#make zoom in plot\n",
    "ax[1].set_title(\"Zoom in Near Expected minima\")\n",
    "\n",
    "#plot function\n",
    "ax[1].plot(xs,ys,'g--')\n",
    "\n",
    "#add slolid line for x and y axis\n",
    "ax[1].axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "ax[1].axvline(x=0, color='black', linestyle='-', linewidth=1)\n",
    "\n",
    "#set zoom in region\n",
    "ax[1].set_xlim(1.7, 2.3)\n",
    "ax[1].set_ylim(-0.5,0.5)\n",
    "\n",
    "#set xlabel/ylabel\n",
    "ax[1].set_xlabel(\"x\")\n",
    "\n",
    "#plot a marker at our found minimum point to verify\n",
    "ax[1].plot(min_gd,0,'mx',ms=10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787b1cf-b241-438c-82a8-020710aca45b",
   "metadata": {},
   "source": [
    "### Activity 4: Optimization with Newton's Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9733b5ad-ace0-43bc-b1f9-cbede779e171",
   "metadata": {},
   "source": [
    "Below we have created an optimization function that uses Newton's Method. Take a moment to examine the code, and verify it performs the expected computation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d934c49-6218-4571-af9e-3f424c4468f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_optimization(f, f_prime, f_double_prime, x0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Find minimum of f(x) using Newton's method.\n",
    "    \n",
    "    Parameters:\n",
    "        f: function to minimize\n",
    "        f_prime: first derivative\n",
    "        f_double_prime: second derivative\n",
    "        x0: initial guess\n",
    "        tol: tolerance for convergence\n",
    "        max_iter: maximum number of iterations\n",
    "    \n",
    "    Returns:\n",
    "        minimum location, function value, and number of iterations\n",
    "    \"\"\"\n",
    "    x = x0\n",
    "    iterations = 0\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        fpx = f_prime(x)\n",
    "        fppx = f_double_prime(x)\n",
    "        \n",
    "        if abs(fpx) < tol:\n",
    "            return x, f(x), iterations\n",
    "        \n",
    "        if fppx == 0:\n",
    "            raise ValueError(\"Second derivative is zero.\")\n",
    "        \n",
    "        x = x - fpx / fppx\n",
    "        iterations += 1\n",
    "    \n",
    "    return x, f(x), iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec441cca-9598-40fa-9e63-26d26bd05843",
   "metadata": {},
   "source": [
    "For this activity, we will use the same function as the earlier example: $g(x)=x^2 - 4x +4$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d6d85-b116-4e2a-a914-fdf4d9163334",
   "metadata": {},
   "source": [
    "Use the Newton Optimization function to find the minimum value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ae964d-95c1-4ce5-8e73-5ddb7f217f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#call function on g(x)\n",
    "min_newton, fmin_newton, iter_newton_opt = newton_optimization(g, g_prime, g_double_prime, 5.0)\n",
    "\n",
    "#print results\n",
    "print(f\"Minimum at: x = {min_newton:.10f}\")\n",
    "print(f\"Function value: g(x) = {fmin_newton:.10f}\")\n",
    "print(f\"Iterations: {iter_newton_opt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616395c1-29b2-4d9d-b0b1-f699822c3f6e",
   "metadata": {},
   "source": [
    "Notice how quickly this converges to the same result!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707b009f-ac92-408b-aacd-501556fb5b59",
   "metadata": {},
   "source": [
    "### Activity 5: Multi-Dimensional Example - Rosenbrock Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da251332-39c3-4c27-af9d-256cddcc99ae",
   "metadata": {},
   "source": [
    "The Rosenbrock function is a commonly used example function in multidimensional optimization. It is defined as:\n",
    "$$\n",
    "f(x,y) = (a-x)^2 + b(y-x)^2\n",
    "$$\n",
    "A visualization is included below showing the case of $a=1, b=100$, which has a minimum at the point $(1,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbcea2c-3f79-4101-85cf-726bf8deec09",
   "metadata": {},
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/6/68/Rosenbrock-contour.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fa9a46-b7de-4a3d-af6b-a08985deb2c1",
   "metadata": {},
   "source": [
    "Below we have defined the Rosenbrock Function as described above, along with its derivatives, which will be needed for our optimization functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ef1b26-2f27-42de-b743-128207a6f34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function\n",
    "def rosenbrock(x):\n",
    "    \"\"\"Rosenbrock function with a = 1, b = 100\"\"\"\n",
    "    return (1 - x[0])**2 + 100 * (x[1] - x[0]**2)**2\n",
    "\n",
    "# Grad representative of 1st derivatives\n",
    "def rosenbrock_grad(x):\n",
    "    \"\"\"Gradient of Rosenbrock function.\"\"\"\n",
    "    dx = -2*(1 - x[0]) - 400*x[0]*(x[1] - x[0]**2)\n",
    "    dy = 200*(x[1] - x[0]**2)\n",
    "    return np.array([dx, dy])\n",
    "\n",
    "# Hessian representative of 2nd derivatives\n",
    "def rosenbrock_hessian(x):\n",
    "    \"\"\"Hessian matrix of Rosenbrock function.\"\"\"\n",
    "    h11 = 2 - 400*x[1] + 1200*x[0]**2\n",
    "    h12 = -400*x[0]\n",
    "    h21 = -400*x[0]\n",
    "    h22 = 200\n",
    "    return np.array([[h11, h12], [h21, h22]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3473387e-280f-4de0-b5bb-2beceba24740",
   "metadata": {},
   "source": [
    "Now we will define a couple of the methods discussed earlier, but adapting them to multideminsional cases. Let's start with Gradent Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f3da73b-422d-44b4-be94-f90f87b1d8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_nd(f, grad_f, x0, learning_rate=0.01, tol=1e-6, max_iter=10000):\n",
    "    \"\"\"\n",
    "    Gradient descent for multidimensional functions.\n",
    "    \n",
    "    Parameters:\n",
    "        f: objective function\n",
    "        grad_f: gradient function\n",
    "        x0: initial point (numpy array)\n",
    "        learning_rate: step size\n",
    "        tol: convergence tolerance\n",
    "        max_iter: maximum iterations\n",
    "    \n",
    "    Returns:\n",
    "        minimum point, function value, iterations, and path history\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    history = [x.copy()]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        x_new = x - learning_rate * grad\n",
    "        history.append(x_new.copy())\n",
    "        \n",
    "        if np.linalg.norm(x_new - x) < tol:\n",
    "            return x_new, f(x_new), i, np.array(history)\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    return x, f(x), max_iter, np.array(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3230b8-1e2a-45c0-ba12-ed3c7cabfdf8",
   "metadata": {},
   "source": [
    "Run our ND Gradient Descent function on Rosenbrock and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a026de-5203-4538-b176-f335c49a64ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define initial guess\n",
    "x0_rosen = np.array([-1.0, -1.0])\n",
    "\n",
    "# run function\n",
    "min_rosen_gd, fmin_rosen_gd, iter_rosen_gd, path_rosen_gd = gradient_descent_nd(\n",
    "    rosenbrock, rosenbrock_grad, x0_rosen, learning_rate=0.001, max_iter=10000\n",
    ")\n",
    "print(f\"Starting point: ({x0_rosen[0]:.2f}, {x0_rosen[1]:.2f})\")\n",
    "print(f\"Minimum found at: ({min_rosen_gd[0]:.6f}, {min_rosen_gd[1]:.6f})\")\n",
    "print(f\"Function value: {fmin_rosen_gd:.10f}\")\n",
    "print(f\"Iterations: {iter_rosen_gd}\")\n",
    "print(f\"True minimum: (1.0, 1.0), f = 0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36502163-660e-4c47-bfa7-2f30ad579088",
   "metadata": {},
   "source": [
    "The result is pretty close to the desired value!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9706c901-7ca2-46c6-9ae0-10ee1ee8d91d",
   "metadata": {},
   "source": [
    "Now let's look at a implementation of Newton's method to compare with the above result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e99061-8793-4728-826c-76646462d84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_nd(f, grad_f, hess_f, x0, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Newton's method for multidimensional optimization.\n",
    "    \n",
    "    Parameters:\n",
    "        f: objective function\n",
    "        grad_f: gradient function\n",
    "        hess_f: Hessian matrix function\n",
    "        x0: initial point\n",
    "        tol: convergence tolerance\n",
    "        max_iter: maximum iterations\n",
    "    \n",
    "    Returns:\n",
    "        minimum point, function value, iterations, and path history\n",
    "    \"\"\"\n",
    "    x = np.array(x0, dtype=float)\n",
    "    history = [x.copy()]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        grad = grad_f(x)\n",
    "        hess = hess_f(x)\n",
    "        \n",
    "        if np.linalg.norm(grad) < tol:\n",
    "            return x, f(x), i, np.array(history)\n",
    "        \n",
    "        # Solve H * delta_x = -grad\n",
    "        try:\n",
    "            delta_x = np.linalg.solve(hess, -grad)\n",
    "            x_new = x + delta_x\n",
    "            history.append(x_new.copy())\n",
    "            x = x_new\n",
    "        except np.linalg.LinAlgError:\n",
    "            print(\"Singular Hessian matrix\")\n",
    "            return x, f(x), i, np.array(history)\n",
    "    \n",
    "    return x, f(x), max_iter, np.array(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0191391-e342-4830-a62c-1452d12e7545",
   "metadata": {},
   "source": [
    "Run our ND Newton's Method function on Rosenbrock and print the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b3eb1a-fad5-4d41-b964-56f2c76f6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# intial guess\n",
    "x0_rosen_newton = np.array([-1.0, -1.0])\n",
    "\n",
    "#run function\n",
    "min_rosen_newton, fmin_rosen_newton, iter_rosen_newton, path_rosen_newton = newton_nd(\n",
    "    rosenbrock, rosenbrock_grad, rosenbrock_hessian, x0_rosen_newton)\n",
    "\n",
    "#print results\n",
    "print(f\"Starting point: ({x0_rosen_newton[0]:.2f}, {x0_rosen_newton[1]:.2f})\")\n",
    "print(f\"Minimum found at: ({min_rosen_newton[0]:.6f}, {min_rosen_newton[1]:.6f})\")\n",
    "print(f\"Function value: {fmin_rosen_newton:.10f}\")\n",
    "print(f\"Iterations: {iter_rosen_newton}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd10adc5-b372-4f5d-93c0-d408f39edfe9",
   "metadata": {},
   "source": [
    "Notice how much faster this converges compared to Gradient Descent! It is a huge advantage to use the 2nd derivative if it is well behaved!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5327fab0-ca40-4cc6-b586-841c3925cb37",
   "metadata": {},
   "source": [
    "Let's look at a plot to better understand these results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58eea73-6906-40b0-9ded-adf6e0b72801",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Create meshgrid for contour plot\n",
    "x_range = np.linspace(-1.5, 1.5, 200)\n",
    "y_range = np.linspace(-1.5, 2.5, 200)\n",
    "X, Y = np.meshgrid(x_range, y_range)\n",
    "Z = np.zeros_like(X)\n",
    "for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "        Z[i, j] = rosenbrock([X[i, j], Y[i, j]])\n",
    "\n",
    "# Plot 1: 3D Surface\n",
    "ax1 = fig1.add_subplot(131, projection='3d')\n",
    "ax1.plot_surface(X, Y, np.log10(Z + 1), cmap='viridis', alpha=0.8)\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('y')\n",
    "ax1.set_zlabel('log₁₀(f + 1)')\n",
    "ax1.set_title('Rosenbrock Function\\n(3D Surface)', fontweight='bold')\n",
    "\n",
    "# Plot 2: Contour with Gradient Descent path\n",
    "ax2 = fig1.add_subplot(132)\n",
    "contour_levels = np.logspace(-1, 3, 30)\n",
    "cs = ax2.contour(X, Y, Z, levels=contour_levels, cmap='viridis', alpha=0.6)\n",
    "ax2.plot(path_rosen_gd[:, 0], path_rosen_gd[:, 1], 'r.-', linewidth=2, \n",
    "         markersize=4, label='Gradient Descent Path')\n",
    "ax2.plot(1, 1, 'g*', markersize=20, label='True Minimum')\n",
    "ax2.plot(x0_rosen[0], x0_rosen[1], 'ro', markersize=10, label='Start')\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('y')\n",
    "ax2.set_title('Gradient Descent Path\\n(Contour Plot)', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "ax2.set_xlim(-1.5, 1.5)\n",
    "ax2.set_ylim(-1.5, 2.5)\n",
    "\n",
    "# Plot 3: Contour with Newton's Method path\n",
    "ax3 = fig1.add_subplot(133)\n",
    "cs = ax3.contour(X, Y, Z, levels=contour_levels, cmap='viridis', alpha=0.6)\n",
    "ax3.plot(path_rosen_newton[:, 0], path_rosen_newton[:, 1], 'b.-', \n",
    "         linewidth=2, markersize=8, label=\"Newton's Method Path\")\n",
    "ax3.plot(1, 1, 'g*', markersize=20, label='True Minimum')\n",
    "ax3.plot(x0_rosen_newton[0], x0_rosen_newton[1], 'bo', markersize=10, label='Start')\n",
    "ax3.set_xlabel('x')\n",
    "ax3.set_ylabel('y')\n",
    "ax3.set_title(\"Newton's Method Path\\n(Contour Plot)\", fontweight='bold')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee64698-e9c6-4c1e-b5e5-9937996284d5",
   "metadata": {},
   "source": [
    "Notice how Gradient Decent follows a relatively smooth path to the minimum, while Netwon's method is more jagged. If you take a moment to think about this, this demonstrates why noisy data can be dangerous in Newton's approach. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb4ff96-9d1f-44e7-a842-2b5e3674999a",
   "metadata": {},
   "source": [
    "As an addendum to this activity, try returning to the cells above and try an different intial guess to see how these functions perform relative to one another. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c3c02e-1805-41ae-bb56-46a0febce25c",
   "metadata": {},
   "source": [
    "### Activity 6: Multi-Dimensional Example - Mass Spring System"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537edcd0-22fa-41eb-805a-091e0495e13e",
   "metadata": {},
   "source": [
    "In this activity, we will look at a mass spring system similar to one we've studied in the past with 2 masses and 3 springs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa22a88-ffe7-4c7e-937e-da5af5a455bb",
   "metadata": {},
   "source": [
    "![](https://www.researchgate.net/profile/Graham-Wood-2/publication/303598579/figure/fig1/AS:416829198618624@1476391233548/Mass-spring-model-of-a-2-DOF-system-consisting-of-two-coupled-resonators.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be21d2f7-6c0d-42da-b430-4f79ad44c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spring_potential(x):\n",
    "    \"\"\"\n",
    "    Potential energy of a two-mass spring system.\n",
    "    x = [x1, x2] are positions of two masses.\n",
    "    Three springs: wall-mass1, mass1-mass2, mass2-wall\n",
    "    Spring constants: k1=1, k2=2, kc=1\n",
    "    Equilibrium positions: 0, 2, 4\n",
    "    \"\"\"\n",
    "    k1, k2, kc = 1.0, 2.0, 1.0\n",
    "    x1, x2 = x[0], x[1]\n",
    "    \n",
    "    # Energy from each spring (1/2 k x²)\n",
    "    E1 = 0.5 * k1 * (x1 - 0)**2      # wall to mass 1\n",
    "    E2 = 0.5 * k2 * (x2 - x1 - 2)**2  # mass 1 to mass 2 (rest length = 2)\n",
    "    E3 = 0.5 * kc * (4 - x2)**2      # mass 2 to wall\n",
    "    \n",
    "    return E1 + E2 + E3\n",
    "\n",
    "def spring_potential_grad(x):\n",
    "    \"\"\"Gradient of spring potential energy.\"\"\"\n",
    "    k1, k2, kc = 1.0, 2.0, 1.0\n",
    "    x1, x2 = x[0], x[1]\n",
    "    \n",
    "    dx1 = k1*(x1 - 0) - k2*(x2 - x1 - 2)\n",
    "    dx2 = k2*(x2 - x1 - 2) - kc*(4 - x2)\n",
    "    \n",
    "    return np.array([dx1, dx2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508fad9-7781-4ca5-93b1-c947601d1cd4",
   "metadata": {},
   "source": [
    "Now we consider this as an optimization problem where we minimize the energy to find the equilibirum position. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec0898c-b88a-4f38-be24-9ee0eaaa443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start with initial guess\n",
    "x0_spring = np.array([0.0, 1.5])\n",
    "\n",
    "#run Gradient Descent\n",
    "min_spring, fmin_spring, iter_spring, path_spring = gradient_descent_nd(\n",
    "    spring_potential, spring_potential_grad, x0_spring, learning_rate=0.1)\n",
    "\n",
    "#print results\n",
    "print(f\"Initial positions: x1={x0_spring[0]:.2f}, x2={x0_spring[1]:.2f}\")\n",
    "print(f\"Equilibrium positions: x1={min_spring[0]:.6f}, x2={min_spring[1]:.6f}\")\n",
    "print(f\"Minimum potential energy: {fmin_spring:.6f}\")\n",
    "print(f\"Iterations: {iter_spring}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be33213-ae03-4c74-80e1-f685f997977c",
   "metadata": {},
   "source": [
    "Create a plot to visualize the situation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ead8a7-d9dc-4c54-9134-78d49b902995",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Create meshgrid for spring potential\n",
    "x1_range = np.linspace(-1, 3, 100)\n",
    "x2_range = np.linspace(1, 5, 100)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "Z_spring = np.zeros_like(X1)\n",
    "for i in range(X1.shape[0]):\n",
    "    for j in range(X1.shape[1]):\n",
    "        Z_spring[i, j] = spring_potential([X1[i, j], X2[i, j]])\n",
    "\n",
    "# Plot 1: 3D Surface\n",
    "ax1 = fig2.add_subplot(131, projection='3d')\n",
    "surf = ax1.plot_surface(X1, X2, Z_spring, cmap='coolwarm', alpha=0.8)\n",
    "ax1.plot(path_spring[:, 0], path_spring[:, 1], \n",
    "         [spring_potential(p) for p in path_spring], \n",
    "         'ko-', linewidth=2, markersize=4, label='Optimization Path')\n",
    "ax1.set_xlabel('x₁ (Mass 1 Position)')\n",
    "ax1.set_ylabel('x₂ (Mass 2 Position)')\n",
    "ax1.set_zlabel('Potential Energy')\n",
    "ax1.set_title('Spring System Potential Energy\\n(3D Surface)', fontweight='bold')\n",
    "\n",
    "# Plot 2: Contour with optimization path\n",
    "ax2 = fig2.add_subplot(132)\n",
    "cs = ax2.contour(X1, X2, Z_spring, levels=20, cmap='coolwarm')\n",
    "ax2.clabel(cs, inline=True, fontsize=8)\n",
    "ax2.plot(path_spring[:, 0], path_spring[:, 1], 'ko-', linewidth=2, \n",
    "         markersize=4, label='Optimization Path')\n",
    "ax2.plot(min_spring[0], min_spring[1], 'g*', markersize=20, label='Equilibrium')\n",
    "ax2.plot(x0_spring[0], x0_spring[1], 'ro', markersize=10, label='Start')\n",
    "ax2.set_xlabel('x₁ (Mass 1 Position)')\n",
    "ax2.set_ylabel('x₂ (Mass 2 Position)')\n",
    "ax2.set_title('Optimization Path\\n(Contour Plot)', fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Physical representation\n",
    "ax3 = fig2.add_subplot(133)\n",
    "ax3.plot([0, min_spring[0]], [0, 0], 'b-', linewidth=3, label='Spring 1',alpha=0.5)\n",
    "ax3.plot([min_spring[0], min_spring[1]], [0, 0], 'r-', linewidth=3, label='Spring 2',alpha=0.5)\n",
    "ax3.plot([min_spring[1], 4], [0, 0], 'g-', linewidth=3, label='Spring 3',alpha=0.5)\n",
    "ax3.plot([0], [0], 'ks', markersize=10, label='Wall')\n",
    "ax3.plot([min_spring[0]], [0], 'ro', markersize=10, label='Mass 1')\n",
    "ax3.plot([min_spring[1]], [0], 'bo', markersize=10, label='Mass 2')\n",
    "ax3.plot([4], [0], 'ks', markersize=10)\n",
    "ax3.set_xlim(-0.5, 4.5)\n",
    "ax3.set_ylim(-0.5, 0.5)\n",
    "ax3.set_xlabel('Position')\n",
    "ax3.set_title('Equilibrium Configuration', fontweight='bold')\n",
    "#ax3.legend(loc='upper right', fontsize=8)\n",
    "ax3.legend(bbox_to_anchor=(1.05, 1), loc='upper left',fontsize=8)\n",
    "\n",
    "ax3.grid(True, alpha=0.3)\n",
    "ax3.set_aspect('equal')\n",
    "\n",
    "#plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc18dfdc-942d-4935-8841-10fb2b2ce0cf",
   "metadata": {},
   "source": [
    "Try re-running the above cells with different start positons for the masses. See how the algorith takes changes, but we still converge to a similar solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df8074-7bf1-44bc-860a-648e846d7038",
   "metadata": {},
   "source": [
    "You can also try changing values of masses and spring constants for different versions of this same type of problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbb4f86-e24d-4053-88b7-41a159b0c77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
